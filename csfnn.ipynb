{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_core = 5\n",
    "num_of_fmap = 7\n",
    "num_of_hidden_unit = 100\n",
    "weights_init = 0.01\n",
    "random_seed = 123123\n",
    "batch_size = 1\n",
    "epoch = 10\n",
    "image_size = 28\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n32000\n[1 0 1 ..., 7 8 0]\n(32000,)\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#data processing\n",
    "train_data = (pd.read_csv(\"~/Developer/kaggle.Digit_Recognizer/datasets/train.csv\")\n",
    "               .iloc[:32000,1:].values).astype('float32')\n",
    "train_label = (pd.read_csv(\"~/Developer/kaggle.Digit_Recognizer/datasets/train.csv\")\n",
    "               .iloc[:32000,0].values).astype('int32')\n",
    "test_data = (pd.read_csv(\"~/Developer/kaggle.Digit_Recognizer/datasets/train.csv\")\n",
    "               .iloc[32000:,1:].values).astype('float32')\n",
    "test_label = (pd.read_csv(\"~/Developer/kaggle.Digit_Recognizer/datasets/train.csv\")\n",
    "               .iloc[32000:,0].values).astype('int32')\n",
    "\n",
    "print(train_data)\n",
    "print(train_data.shape[0])\n",
    "print(train_label)\n",
    "print(train_label.shape)\n",
    "print(test_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ..., 0 0 0]\n [1 0 0 ..., 0 0 0]\n [0 1 0 ..., 0 0 0]\n ..., \n [0 0 0 ..., 1 0 0]\n [0 0 0 ..., 0 1 0]\n [1 0 0 ..., 0 0 0]]\n(32000, 10)\n"
     ]
    }
   ],
   "source": [
    "#one hot encode label\n",
    "num_of_labels = train_label.shape[0]\n",
    "encoded_labels = np.zeros((num_of_labels, 10),dtype=np.int)\n",
    "for i in range(0, num_of_labels):\n",
    "    encoded_labels[i][train_label[i]] = 1\n",
    "train_label = encoded_labels\n",
    "print(train_label)\n",
    "print(train_label.shape)\n",
    "# regularlize data\n",
    "train_data /= 255\n",
    "test_data /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 25)\n(1008, 100)\n(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#weights init\n",
    "rand.seed(a=123123)\n",
    "\n",
    "in_to_conv_weights = np.zeros((num_of_fmap,conv_core*conv_core),dtype=np.float)\n",
    "for i in range(0, in_to_conv_weights.shape[0]):\n",
    "    for j in range(0, in_to_conv_weights.shape[1]):\n",
    "        in_to_conv_weights[i][j] = weights_init*rand.random()\n",
    "print(in_to_conv_weights.shape)\n",
    "#[[map1No1*1, map1No1*2, ...map1No28*28]\n",
    "#[map2No1*1, map2No1*2, ...map2No28*28]\n",
    "#...\n",
    "#[map6No1*1, map6No1*2, ...map6No28*28]]\n",
    "\n",
    "#after submapping\n",
    "\n",
    "#[[map1No1*1, map1No1*2, ...map1No14*14]\n",
    "#[map2No1*1, map2No1*2, ...map2No14*14]\n",
    "#...\n",
    "#[map6No1*1, map6No1*2, ...map6No14*14]]\n",
    "\n",
    "#to input: to one line: 6*14*14 para\n",
    "hid_bias = np.ones((batch_size, num_of_hidden_unit), dtype=np.float)\n",
    "hid_bias *= weights_init * rand.random()\n",
    "out_bias = np.ones((batch_size, 10), dtype=np.float)\n",
    "out_bias *= weights_init * rand.random()\n",
    "\n",
    "in_to_hid_weights = np.zeros((int(num_of_fmap*((28-conv_core+1)/2)*((28-conv_core+1)/2)),num_of_hidden_unit),dtype=np.float)\n",
    "for i in range(0, in_to_hid_weights.shape[0]):\n",
    "    for j in range(0, in_to_hid_weights.shape[1]):\n",
    "        in_to_hid_weights[i][j] = weights_init*rand.random()\n",
    "print(in_to_hid_weights.shape)\n",
    "\n",
    "hid_to_out_weights = np.zeros((num_of_hidden_unit, train_label.shape[1]), dtype=np.float)\n",
    "for i in range(0, hid_to_out_weights.shape[0]):\n",
    "    for j in range(0, hid_to_out_weights.shape[1]):\n",
    "        hid_to_out_weights[i][j] = weights_init*rand.random()\n",
    "print(hid_to_out_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n[[ 0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.]]\n(1, 7, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "#mini batch frop\n",
    "#print(train_data)\n",
    "##l1\n",
    "train_data_batch = train_data[:1]\n",
    "train_label_batch = train_label[:1]\n",
    "\n",
    "two_d_input = train_data_batch.reshape(train_data_batch.shape[0],image_size, image_size)\n",
    "print(two_d_input.shape)\n",
    "print((two_d_input[0][0:5,0:5]))\n",
    "\n",
    "conved_input = np.zeros((train_data_batch.shape[0],num_of_fmap,image_size-conv_core+1,image_size-conv_core+1),dtype=np.float)\n",
    "conved_input_maxloc = np.zeros((train_data_batch.shape[0],num_of_fmap,image_size-conv_core+1,image_size-conv_core+1),dtype=np.float)\n",
    "for i in range(0, train_data_batch.shape[0]):\n",
    "    for j in range(0, num_of_fmap):\n",
    "        for k in range(0,image_size-conv_core+1):\n",
    "            for l in range(0, image_size-conv_core+1):\n",
    "                conved_input[i][j][k][l] = sum(sum(two_d_input[i][k:k+conv_core,l:l+conv_core]*(in_to_conv_weights[j].\n",
    "                                                                                                reshape(conv_core,conv_core)))) \n",
    "                \n",
    "##l2(max)\n",
    "# print(conved_input)\n",
    "sampled_input = np.zeros((train_data_batch.shape[0],num_of_fmap,int((image_size-conv_core+1)/2),int((image_size-conv_core+1)/2)),dtype=np.float)\n",
    "for i in range(0, train_data_batch.shape[0]):\n",
    "    for j in range(0, num_of_fmap):\n",
    "        for k in range(0,int(conved_input.shape[2]/2)):\n",
    "            for l in range(0, int(conved_input.shape[3]/2)):\n",
    "                sampled_input[i][j][k][l]= np.amax(conved_input[i][j][2*k:2*k+2,2*l:2*l+2])\n",
    "                agmx = np.argmax(conved_input[i][j][2*k:2*k+2,2*l:2*l+2])\n",
    "                conved_input_maxloc[i][j][2*k+int(agmx/2)][2*l+agmx%2] = 1\n",
    "#print(np.amax(conved_input[0][0][2*0:2*0+2,2*0:2*0+2]))\n",
    "#print(sampled_input[0][0])\n",
    "#print(conved_input[0][0])\n",
    "#print(conved_input_maxloc[0][0])\n",
    "print(sampled_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1008)\n(1, 1008)\n(1, 7, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "def one_layer_training_with_bias_api(alpha, train_batch, train_label_batch, in_to_hid_weights, hid_bias,\n",
    "                                     hid_to_out_weights,out_bias):\n",
    "# fprop\n",
    "    hid_state = np.dot(train_batch, in_to_hid_weights)\n",
    "    hid_state = hid_state + hid_bias\n",
    "    out_state = np.dot(hid_state, hid_to_out_weights)\n",
    "    out_state = out_state + out_bias\n",
    "    out_state = 1 / (1 + np.exp(-out_state))\n",
    "\n",
    "    # bprop:err\n",
    "    err = out_state - train_label_batch\n",
    "    CE = (err * err / 2).sum(axis=1)\n",
    "    d_Out = out_state * (1 - out_state) * err\n",
    "    d_hid_to_out = np.dot(hid_state.transpose(), d_Out)\n",
    "    d_hid = np.dot(d_Out, hid_to_out_weights.transpose())\n",
    "    d_in_to_hid = np.dot(train_batch.transpose(), d_hid)\n",
    "    d_in = np.dot(d_hid, in_to_hid_weights.transpose())\n",
    "\n",
    "    # update weights\n",
    "    del_hid_to_out = -1 * alpha * d_hid_to_out / batch_size\n",
    "    del_in_to_hid = -1 * alpha * d_in_to_hid / batch_size\n",
    "    hid_to_out_weights += del_hid_to_out\n",
    "    in_to_hid_weights += del_in_to_hid\n",
    "\n",
    "    hid_bias += -1 * alpha * d_hid\n",
    "    out_bias += -1 * alpha * d_Out\n",
    "\n",
    "    return in_to_hid_weights,hid_bias,hid_to_out_weights,out_bias,d_in\n",
    "\n",
    "\n",
    "##mini batch l3(ffnn)\n",
    "\n",
    "\n",
    "\n",
    "oned_fnn_in = sampled_input.reshape(sampled_input.shape[0],sampled_input.shape[1]*sampled_input.shape[2]*sampled_input.shape[3])\n",
    "print(oned_fnn_in.shape)\n",
    "(in_to_hid_weights,hid_bias,hid_to_out_weights,out_bias,d_in) = one_layer_training_with_bias_api\\\n",
    "    (alpha, oned_fnn_in,train_label_batch, in_to_hid_weights, hid_bias, hid_to_out_weights,out_bias)\n",
    "print(d_in.shape)\n",
    "\n",
    "##bp conv\n",
    "d_in = d_in.reshape(d_in.shape[0],num_of_fmap,int((28-conv_core+1)/2),int((28-conv_core+1)/2))\n",
    "expd_d_in = np.zeros((d_in.shape[0],d_in.shape[1],d_in.shape[2]*2,d_in.shape[3]*2),dtype=np.float)\n",
    "print(d_in.shape)\n",
    "kr = [[1,1],[1,1]]\n",
    "for i in range(0, train_data_batch.shape[0]):\n",
    "    for j in range(0, num_of_fmap):\n",
    "        expd_d_in[i][j] = np.kron(d_in[i][j],kr)\n",
    "#print(expd_d_in[0][0])\n",
    "expd_d_in = expd_d_in*conved_input_maxloc\n",
    "#print(expd_d_in[0][5])\n",
    "#print(train_data_batch)\n",
    "\n",
    "d_cov_w = np.zeros((d_in.shape[1],conv_core,conv_core),dtype=np.float)\n",
    "for i in range(0, train_data_batch.shape[0]):\n",
    "    for j in range(0, num_of_fmap):\n",
    "        for k in range(0,conv_core):\n",
    "            for l in range(0,conv_core):\n",
    "                d_cov_w[j][k][l] += sum(sum(expd_d_in[i][j]*two_d_input[i][k:k+expd_d_in.shape[2],l:l+expd_d_in.shape[3]]))\n",
    "#print(d_cov_w)\n",
    "#print(in_to_conv_weights)\n",
    "#print(in_to_conv_weights[5])\n",
    "in_to_conv_weights += -alpha*(d_cov_w.reshape(d_cov_w.shape[0],d_cov_w.shape[1]*d_cov_w.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######train########\n",
    "for ep in range(0,epoch):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}